# Machine Learning Researcher
I am a PhD candidate at Purdue University where I am advised by Professor Kaushik Roy. During my PhD, I spent time at the Memory Solution Team at GlobalFoundries as a research intern.

[Email](mailto:gsaha@purdue.edu)  |  [Resume](/assets/docs/GobindaSaha_Resume.pdf)  |  [LinkedIn](https://www.linkedin.com/in/gobinda-saha) | [Google Scholar](https://scholar.google.com/citations?user=Y7I-7EsAAAAJ&hl=en)  |  [Github](https://github.com/sahagobinda) 

<!---
#### Technical Skills: Python, PyTorch, AWS, MATLAB

## Education
- Ph.D., Electrical and Computer Engineering | Purdue University (_October 2023_)								       		
- M.S., Electrical and Electronic Engineering	| Bangladesh University of Engineering and Technology (_Aughts 2015_)	 			        		
- B.S., Electrical and Electronic Engineering | Bangladesh University of Engineering and Technology (_February 2013_)

## Work Experience
**Graduate Research Assistant @ Purdue University (_August 2017 - Present_)**
- contribution

**Research Intern, Memory Solution Team @ GlobalFoundries, USA (_June 2019 - August 2019_)**
- contribution
-->

## News
- **02/11/2023**: Presented our paper [Continual Learning with Scaled Gradient Projection](https://ojs.aaai.org/index.php/AAAI/article/view/26157) at [AAAI 2023]([https://wacv2023.thecvf.com/node/174](https://aaai-23.aaai.org/)), Washington, DC, USA. 
- **01/07/2023**: Our paper [Saliency Guided Experience Packing for Replay in Continual Learning](https://openaccess.thecvf.com/content/WACV2023/html/Saha_Saliency_Guided_Experience_Packing_for_Replay_in_Continual_Learning_WACV_2023_paper.html) made the [WACV 2023 Award finalist](https://wacv2023.thecvf.com/node/174) list.
- **07/10/2022**: Our perspective on [A cross-layer approach to cognitive computing](https://dl.acm.org/doi/abs/10.1145/3489517.3530642) was presented at ACM/IEEE Design Automation Conference 2022.
- **01/12/2021**: Our paper [Gradient Projection Memory for Continual Learning](https://openreview.net/forum?id=3AOj0RCNC2) is selected for Oral presentation at [ICLR 2021](https://iclr.cc/Conferences/2021).

## Publications 
### [Continual Learning with Scaled Gradient Projection](blue) 

**Gobinda Saha**, Kaushik Roy<br>
*AAAI Conference on Artificial Intelligence (**AAAI 2023**)*<br> 
[[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/26157) [[Code]](https://github.com/sahagobinda/SGP) [[Talk Video]](/assets/videos/sgp_saha_aaai_presentation.mp4) 

- A scaled gradient projection algorithm for balancing stability and plasticity during continual learning.
- Attained up to 2% higher accuracy in image classification and ~12% more reward in reinforcement learning (Atari games) tasks than SOTA with minimal forgetting. 

<img src="/assets/Images/sgp.png" alt="SGP overview" width="700"/>


### [Saliency Guided Experience Packing for Replay in Continual Learning](blue)

**Gobinda Saha**, Kaushik Roy<br>
*IEEE/CVF Winter Conference on Applications of Computer Vision (**WACV 2023**)*<br>
[[Paper]](https://openaccess.thecvf.com/content/WACV2023/html/Saha_Saliency_Guided_Experience_Packing_for_Replay_in_Continual_Learning_WACV_2023_paper.html) [[Code]](https://github.com/sahagobinda/EPR) [[Talk Video]](/assets/videos/wacv_presentation.wmv)

- A new experience replay method for continual learning where explainable AI (XAI) tools such as saliency maps are used for memory selection.
- Attained up to 5% accuracy improvement over SOTA on online continual object classification benchmarks with tiny episodic memories.

<img src="/assets/Images/epr.png" alt="EPR overview" width="450"/>


### [Gradient Projection Memory for Continual Learning](blue)

**Gobinda Saha**, Isha Garg, Kaushik Roy<br>
*International Conference on Learning Representations (**ICLR 2021**)* (**Oral - top 1% paper**)<br>
[[Paper]](https://openreview.net/forum?id=3AOj0RCNC2) [[Code]](https://github.com/sahagobinda/GPM) [[Talk Video]](https://slideslive.com/38953615/gradient-projection-memory-for-continual-learning?ref=account-84503-popular)

- A novel orthogonal gradient descent algorithm for forget-free continual learning in deep neural networks. 
- Obtained near zero forgetting on continual object classification tasks.  

<img src="/assets/Images/gpm.png" alt="GPM overview" width="700"/>


### [SPACE: Structured Compression and Sharing of Representational Space for Continual Learning](blue)

**Gobinda Saha**, Isha Garg, Aayush Ankit, Kaushik Roy<br>
*IEEE Access 2021*<br> 
[[Paper]](https://ieeexplore.ieee.org/document/9605653) [[Code]](https://github.com/sahagobinda/CL_PCA) 

- A PCA-driven network pruning and growth method for forget-free continual learning. 
- Achieved zero forgetting with up to 5x energy efficiency during inference due to emerging sparsity.

<img src="/assets/Images/space.png" alt="SPACE overview" width="600"/>

---

### [CoDeC: Communication-Efficient Decentralized Continual Learning](blue)

Sakshi Choudhary, Sai Aparna Aketi, **Gobinda Saha**, Kaushik Roy<br>
*Preprint (arxiv)* [[Paper]](https://arxiv.org/abs/2303.15378) 


- A decentralized continual learning algorithm that combines orthogonal gradient projections with gossip-averaging among distributed agents.
- Achieved SOTA accuracy on 3 image classification benchmarks with up to 4.8x reduced communication cost ensuring  inter-agent data privacy.  

<img src="/assets/Images/codec.png" alt="CoDeC overview" width="600"/>
